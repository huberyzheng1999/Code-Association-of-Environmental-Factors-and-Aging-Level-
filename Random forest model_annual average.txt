import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from scipy.stats import spearmanr
import warnings
import os

warnings.filterwarnings('ignore')

DATA_PATH = 'C:/â€¦â€¦'
SAVE_IMPORTANCE_PATH = 'C:/â€¦â€¦'
SAVE_PLOT_PATH = 'C:/â€¦â€¦'
SAVE_VALIDATION_SUMMARY = 'C:/â€¦â€¦'

os.makedirs(SAVE_PLOT_PATH, exist_ok=True)

ID_VARS = ['pac', 'Year', 'Urb']
TARGET_VARS = ['LA65', 'LA85']
URBAN_GROUPS = ['ä½åŸå¸‚åŒ–ç»„', 'ä¸­åŸå¸‚åŒ–ç»„', 'é«˜åŸå¸‚åŒ–ç»„']
QUANTILE_THRESHOLD = 0.01

GROUP_FEATURES_CONFIG = {
    'ä½åŸå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
    'ä½åŸå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
    'ä¸­åŸå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
    'ä¸­åŸå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
    'é«˜åŸå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
    'é«˜åŸå¸‚åŒ–ç»„_LA85': [â€¦â€¦]
}

GROUP_STRICT_PARAMS = {
    'ä½åŸå¸‚åŒ–ç»„': {
        'n_estimators': 200,
        'max_depth': 9,  
        'min_samples_split': 12,  
        'min_samples_leaf': 6,  
        'max_features': 'sqrt',  
        'random_state': 42,
        'n_jobs': -1,
        'oob_score': True
    },
    'ä¸­åŸå¸‚åŒ–ç»„': {
        'n_estimators': 200,
        'max_depth': 9,  
        'min_samples_split': 15,  
        'min_samples_leaf': 8,  
        'max_features': 0.7,  
        'random_state': 42,
        'n_jobs': -1,
        'oob_score': True
    },
    'é«˜åŸå¸‚åŒ–ç»„': {
        'n_estimators': 200,
        'max_depth': 10,  
        'min_samples_split': 10,  
        'min_samples_leaf': 5,  
        'max_features': 'sqrt',  
        'random_state': 42,
        'n_jobs': -1,
        'oob_score': True
    }
}

REPEAT_TIMES = 3
RANDOM_SEEDS = [42, 123, 456]
MIN_TRAIN_R2 = 0.7

def load_data(path):

    df = pd.read_excel(path)
    print(f"åŸå§‹æ•°æ®åˆ—å: {df.columns.tolist()}")
    print(f"æ£€æŸ¥æ˜¯å¦æœ‰'Urb'åˆ—: {'Urb' in df.columns}")

    if 'Urb' in df.columns:
        df['Urb_Group'] = pd.cut(df['Urb'], bins=[-1, 3, 7, 10], labels=URBAN_GROUPS)
        print(f"åŸå¸‚åŒ–åˆ†ç»„åˆ›å»ºå®Œæˆï¼Œå„ç»„æ ·æœ¬é‡:")
        print(df['Urb_Group'].value_counts())
    else:
        print("é”™è¯¯ï¼šæ•°æ®ä¸­æ²¡æœ‰'Urb'åˆ—")
        possible_cols = [col for col in df.columns if 'urb' in col.lower() or 'åŸå¸‚åŒ–' in col]
        if possible_cols:
            print(f"æ‰¾åˆ°å¯èƒ½çš„åˆ—: {possible_cols}")
            df['Urb_Group'] = pd.cut(df[possible_cols[0]], bins=[-1, 3, 7, 10], labels=URBAN_GROUPS)
        else:
            raise KeyError("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°åŸå¸‚åŒ–ç›¸å…³çš„åˆ—")

    all_possible_features = list(set(sum(GROUP_FEATURES_CONFIG.values(), [])))
    all_needed_cols = ID_VARS + TARGET_VARS + all_possible_features + ['Urb_Group']
    existing_cols = [col for col in all_needed_cols if col in df.columns]
    df = df[existing_cols]

    print(f"åˆå¹¶æ•°æ®åŠ è½½å®Œæˆï¼šå…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")
    return df


def handle_outliers(df, features, group_col='Urb_Group', quantile=0.01):
    df_out = df.copy()
    if group_col not in df_out.columns:
        print(f"è­¦å‘Šï¼šæ•°æ®ä¸­æ²¡æœ‰'{group_col}'åˆ—ï¼Œè·³è¿‡åˆ†ç»„å¤„ç†")
        return df_out

    for group in df_out[group_col].unique():
        if pd.isna(group):
            continue
        group_mask = df_out[group_col] == group
        for feat in features:
            if feat not in df_out.columns:
                continue
            group_data = df_out.loc[group_mask, feat]
            if len(group_data) > 0:
                lower = group_data.quantile(quantile)
                upper = group_data.quantile(1 - quantile)
                df_out.loc[group_mask, feat] = group_data.clip(lower=lower, upper=upper)
    print(f"æç«¯å€¼ç¼©å°¾å¤„ç†å®Œæˆï¼ˆç‰¹å¾æ•°ï¼š{len(features)}ï¼‰")
    return df_out


def impute_missing(df, features):
    numeric_features = [f for f in features if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]
    if len(numeric_features) == 0:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æ•°å€¼å‹ç‰¹å¾è¿›è¡Œæ’è¡¥")
        return df

    imputer = IterativeImputer(random_state=42, max_iter=10)
    df_imputed = df.copy()
    df_imputed[numeric_features] = imputer.fit_transform(df_imputed[numeric_features])
    print(f"ç¼ºå¤±å€¼è¿­ä»£æ’è¡¥å®Œæˆï¼Œå¤„ç†äº†{len(numeric_features)}ä¸ªç‰¹å¾")
    return df_imputed


def calculate_fitting_metrics(model, X, y):
    y_pred = model.predict(X)
    r2 = np.round(r2_score(y, y_pred), 4).item()
    if r2 < MIN_TRAIN_R2:
        print(f"âš ï¸ è®­ç»ƒé›†RÂ²={r2}ï¼œ{MIN_TRAIN_R2}ï¼Œéœ€å¾®è°ƒå‚æ•°")
    mae = np.round(mean_absolute_error(y, y_pred), 4).item()
    mse = np.round(mean_squared_error(y, y_pred), 4).item()
    return {'è®­ç»ƒé›†RÂ²': r2, 'MAE': mae, 'MSE': mse}

def calculate_importance_consistency(X, y, features, group, n_repeats=REPEAT_TIMES, seeds=RANDOM_SEEDS):
    importance_rank_list = []
    for seed in seeds:
        params = GROUP_STRICT_PARAMS[group].copy()
        params['random_state'] = seed
        rf = RandomForestRegressor(**params)
        rf.fit(X, y)
        imp_df = pd.DataFrame({'å˜é‡å': features, 'é‡è¦æ€§': rf.feature_importances_})
        imp_df = imp_df.sort_values('é‡è¦æ€§', ascending=False).reset_index(drop=True)
        importance_rank_list.append(imp_df['å˜é‡å'].tolist())

    consistency_scores = []
    for i in range(n_repeats):
        for j in range(i + 1, n_repeats):
            rank_i = pd.Series(range(1, len(features) + 1), index=importance_rank_list[i])
            rank_j = pd.Series(range(1, len(features) + 1), index=importance_rank_list[j])

            rank_i_vals = rank_i[features].values
            rank_j_vals = rank_j[features].values
            valid_mask = ~(np.isnan(rank_i_vals) | np.isnan(rank_j_vals))

            if not np.any(valid_mask):
                corr = np.nan
            else:
                res = spearmanr(rank_i_vals[valid_mask], rank_j_vals[valid_mask])
                corr = res.statistic

            consistency_scores.append(np.round(corr, 4).item() if not np.isnan(corr) else np.nan)

    valid_scores = [s for s in consistency_scores if not np.isnan(s)]
    avg_consistency = np.round(np.mean(valid_scores), 4).item() if valid_scores else np.nan
    return {'é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰': avg_consistency}

def adjust_params_for_overfitting(X, y, group, features):
    strict_params = GROUP_STRICT_PARAMS[group].copy()
    rf_strict = RandomForestRegressor(**strict_params)
    rf_strict.fit(X, y)

    train_metrics = calculate_fitting_metrics(rf_strict, X, y)
    train_r2 = train_metrics['è®­ç»ƒé›†RÂ²']
    oob_r2 = np.round(rf_strict.oob_score_, 4) if hasattr(rf_strict, 'oob_score_') else 0
    overfit_gap = np.round(train_r2 - oob_r2, 4)

    print(f"ğŸ” {group}åˆå§‹ä¸¥æ ¼å‚æ•°ï¼šè®­ç»ƒé›†RÂ²={train_r2}ï¼ŒOOB-RÂ²={oob_r2}ï¼Œè¿‡æ‹Ÿåˆå·®å€¼={overfit_gap}")

    if train_r2 >= MIN_TRAIN_R2:
        print(f"âœ… {group}ä¸¥æ ¼å‚æ•°æ»¡è¶³è®­ç»ƒé›†RÂ²â‰¥{MIN_TRAIN_R2}ï¼Œä½¿ç”¨è¯¥å‚æ•°é™è¿‡æ‹Ÿåˆ")
        return strict_params, rf_strict, overfit_gap

    relax_steps = [
        {'min_samples_split': strict_params['min_samples_split'] - 3,
         'min_samples_leaf': strict_params['min_samples_leaf'] - 2},
        {'max_depth': strict_params['max_depth'] + 1, 'min_samples_split': strict_params['min_samples_split'] - 4},
        {'max_depth': strict_params['max_depth'] + 2, 'min_samples_leaf': strict_params['min_samples_leaf'] - 3}
    ]

    final_params = strict_params.copy()
    final_rf = rf_strict
    final_gap = overfit_gap

    for step, relax in enumerate(relax_steps):
        temp_params = strict_params.copy()
        temp_params.update(relax)
        temp_params['min_samples_split'] = max(5, temp_params['min_samples_split'])
        temp_params['min_samples_leaf'] = max(2, temp_params['min_samples_leaf'])
        temp_params['max_depth'] = min(12, temp_params['max_depth'])

        rf_temp = RandomForestRegressor(**temp_params)
        rf_temp.fit(X, y)
        temp_train_metrics = calculate_fitting_metrics(rf_temp, X, y)
        temp_train_r2 = temp_train_metrics['è®­ç»ƒé›†RÂ²']
        temp_oob_r2 = np.round(rf_temp.oob_score_, 4) if hasattr(rf_temp, 'oob_score_') else 0
        temp_gap = np.round(temp_train_r2 - temp_oob_r2, 4)

        print(f"ğŸ”§ {group}æ”¾å®½æ­¥éª¤{step + 1}ï¼šå‚æ•°{relax} â†’ è®­ç»ƒé›†RÂ²={temp_train_r2}ï¼Œè¿‡æ‹Ÿåˆå·®å€¼={temp_gap}")

        if temp_train_r2 >= MIN_TRAIN_R2:
            final_params = temp_params
            final_rf = rf_temp
            final_gap = temp_gap
            print(f"âœ… {group}æ”¾å®½åè®­ç»ƒé›†RÂ²â‰¥{MIN_TRAIN_R2}ï¼Œè¿‡æ‹Ÿåˆå·®å€¼={temp_gap}")
            break

    return final_params, final_rf, final_gap

def filter_features_by_importance(X, y, features, strat_key, min_importance_threshold=0.02):
    if "ä½åŸå¸‚åŒ–ç»„" in strat_key:
        threshold = 0.01
    elif "LA65" in strat_key and ("ä¸­åŸå¸‚åŒ–ç»„" in strat_key or "é«˜åŸå¸‚åŒ–ç»„" in strat_key):
        threshold = 0.02
    else:
        threshold = 0.015

    temp_rf = RandomForestRegressor(**GROUP_STRICT_PARAMS[strat_key.split('_')[0]])
    temp_rf.fit(X, y)
    imp_df = pd.DataFrame({
        'å˜é‡å': features,
        'é‡è¦æ€§': temp_rf.feature_importances_,
        'é‡è¦æ€§å æ¯”(%)': temp_rf.feature_importances_ * 100
    })

    must_keep_features = ['PM25', 'Medical_personnel', 'Resident_savings', 'GDP']
    must_keep = imp_df[imp_df['å˜é‡å'].isin(must_keep_features)]['å˜é‡å'].tolist()
    filter_imp = imp_df[imp_df['é‡è¦æ€§'] >= threshold]['å˜é‡å'].tolist()
    final_features = list(set(filter_imp + must_keep))
    final_features = [f for f in final_features if f in X.columns]

    if len(final_features) < 3:
        final_features = available_features[:5]
        print(f"âš ï¸ {strat_key}ç­›é€‰åç‰¹å¾è¿‡å°‘ï¼Œå…œåº•ä¿ç•™{len(final_features)}ä¸ª")

    print(f"ğŸ” {strat_key}å˜é‡ç­›é€‰ï¼šåŸå§‹{len(features)}ä¸ª â†’ ç­›é€‰å{len(final_features)}ä¸ªï¼ˆé˜ˆå€¼{threshold * 100}%ï¼‰")
    return final_features, imp_df

def train_stratified_rf_with_validation(df, target, group_col='Urb_Group'):
    all_importance = []
    validation_summary = []

    for group in URBAN_GROUPS:
        strat_key = f"{group}_{target}"
        if strat_key not in GROUP_FEATURES_CONFIG:
            print(f"âš ï¸ åˆ†å±‚{strat_key}æ— ç‰¹å¾é…ç½®ï¼Œè·³è¿‡")
            continue

        strat_features = GROUP_FEATURES_CONFIG[strat_key]
        available_features = [f for f in strat_features if f in df.columns]
        if len(available_features) == 0:
            print(f"âš ï¸ {strat_key}æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—ï¼Œè·³è¿‡")
            continue

        group_data = df[df[group_col] == group].copy()
        if len(group_data) < 30:
            print(f"âš ï¸ {strat_key}æ ·æœ¬é‡ä¸è¶³{len(group_data)}ï¼Œè·³è¿‡")
            continue
        group_data = handle_outliers(group_data, available_features)
        group_data = impute_missing(group_data, available_features)

        filtered_features, imp_df_temp = filter_features_by_importance(
            group_data[available_features], group_data[target], available_features, strat_key
        )
        X = group_data[filtered_features].copy()
        y = group_data[target].copy()

        final_params, rf_main, overfit_gap = adjust_params_for_overfitting(X, y, group, filtered_features)

        oob_r2 = np.round(rf_main.oob_score_, 4).item() if (
                hasattr(rf_main, 'oob_score_') and not np.isnan(rf_main.oob_score_)) else np.nan
        fitting_metrics = calculate_fitting_metrics(rf_main, X, y)
        consistency_metrics = calculate_importance_consistency(X, y, filtered_features, group)

        imp_df = pd.DataFrame({
            'åˆ†å±‚': strat_key,
            'åŸå¸‚åŒ–åˆ†ç»„': group,
            'å› å˜é‡': target,
            'å˜é‡å': filtered_features,
            'å˜é‡é‡è¦æ€§': rf_main.feature_importances_,
            'é‡è¦æ€§å æ¯”(%)': np.round(rf_main.feature_importances_ * 100, 2),
            'OOB-RÂ²': oob_r2,
            'è®­ç»ƒé›†RÂ²': fitting_metrics['è®­ç»ƒé›†RÂ²'],
            'è¿‡æ‹Ÿåˆå·®å€¼': overfit_gap,
            'MAE': fitting_metrics['MAE'],
            'ä½¿ç”¨å‚æ•°': str(final_params)
        })
        imp_df = imp_df.sort_values('å˜é‡é‡è¦æ€§', ascending=False).reset_index(drop=True)
        all_importance.append(imp_df)

        val_summary = {
            'åˆ†å±‚': strat_key,
            'åŸå¸‚åŒ–åˆ†ç»„': group,
            'å› å˜é‡': target,
            'æ ·æœ¬é‡': len(group_data),
            'ä½¿ç”¨ç‰¹å¾æ•°': len(filtered_features),
            'OOB-RÂ²': oob_r2,
            'è®­ç»ƒé›†RÂ²': fitting_metrics['è®­ç»ƒé›†RÂ²'],
            'è¿‡æ‹Ÿåˆå·®å€¼': overfit_gap,
            'MAE': fitting_metrics['MAE'],
            'MSE': fitting_metrics['MSE'],
            'å˜é‡é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰': consistency_metrics['é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰'],
            'æœ€ç»ˆä½¿ç”¨å‚æ•°': str(final_params)
        }
        validation_summary.append(val_summary)

        plot_top_importance(imp_df.head(15), strat_key, target)
        print(
            f"âœ… {strat_key}è®­ç»ƒå®Œæˆï¼šè®­ç»ƒé›†RÂ²={fitting_metrics['è®­ç»ƒé›†RÂ²']}ï¼ŒOOB-RÂ²={oob_r2}ï¼Œè¿‡æ‹Ÿåˆå·®å€¼={overfit_gap}")

    return pd.concat(all_importance, ignore_index=True) if all_importance else pd.DataFrame(), \
        pd.DataFrame(validation_summary) if validation_summary else pd.DataFrame()


def plot_top_importance(importance, strat_key, target):

    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.figure(figsize=(12, 8))
    bars = plt.barh(importance['å˜é‡å'][::-1], importance['é‡è¦æ€§å æ¯”(%)'][::-1], color='#1f77b4')
    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.1, bar.get_y() + bar.get_height() / 2,
                 f'{width:.1f}%', ha='left', va='center', fontsize=9)
    plt.title(f'Variable Importance (Merged Data) - {strat_key}', fontsize=14, fontweight='bold')
    plt.xlabel('Importance (%)', fontsize=12)
    plt.ylabel('Variables', fontsize=12)
    plt.tight_layout()

    safe_strat = strat_key.replace('/', '_').replace('\\', '_').replace(' ', '_')
    safe_target = target.replace('/', '_').replace('\\', '_')
    filename = f'{SAVE_PLOT_PATH}{safe_strat}_importance.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"å›¾è¡¨å·²ä¿å­˜: {filename}")

def main_stratified_rf():
    df = load_data(DATA_PATH)
    print(f"\næ•°æ®æ¦‚è§ˆ: {df.shape}")

    all_importance = []
    all_validation = []

    for target in TARGET_VARS:
        print(f"\n=== å¼€å§‹å¤„ç†å› å˜é‡ï¼š{target} ===")
        if target not in df.columns:
            print(f"âš ï¸ ç›®æ ‡å˜é‡{target}ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        imp_df, val_df = train_stratified_rf_with_validation(df, target)
        if not imp_df.empty:
            all_importance.append(imp_df)
        if not val_df.empty:
            all_validation.append(val_df)

    if all_importance:
        final_importance = pd.concat(all_importance, ignore_index=True)
        final_importance.to_excel(SAVE_IMPORTANCE_PATH, index=False)
        print(f"\nğŸ‰ åˆå¹¶æ•°æ®å˜é‡é‡è¦æ€§ç»“æœå·²ä¿å­˜è‡³ï¼š{SAVE_IMPORTANCE_PATH}")
    else:
        print("\nâš ï¸ æ²¡æœ‰ç”Ÿæˆå˜é‡é‡è¦æ€§ç»“æœ")

    if all_validation:
        final_validation = pd.concat(all_validation, ignore_index=True)
        final_validation.to_excel(SAVE_VALIDATION_SUMMARY, index=False)
        print(f"ğŸ‰ åˆå¹¶æ•°æ®æ¨¡å‹éªŒè¯æ±‡æ€»è¡¨å·²ä¿å­˜è‡³ï¼š{SAVE_VALIDATION_SUMMARY}")
        print("\n=== æ¨¡å‹éªŒè¯æ±‡æ€»é¢„è§ˆï¼ˆé‡ç‚¹çœ‹ï¼šè¿‡æ‹Ÿåˆå·®å€¼ï¼‰===")
        print(final_validation[['åˆ†å±‚', 'æ ·æœ¬é‡', 'ä½¿ç”¨ç‰¹å¾æ•°', 'OOB-RÂ²', 'è®­ç»ƒé›†RÂ²',
                                'è¿‡æ‹Ÿåˆå·®å€¼', 'å˜é‡é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰']].head())
    else:
        print("\nâš ï¸ æœªç”Ÿæˆæ¨¡å‹éªŒè¯æ±‡æ€»è¡¨")


if __name__ == '__main__':
    main_stratified_rf()
