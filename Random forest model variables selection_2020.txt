import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from scipy.stats import spearmanr
import warnings
import os

warnings.filterwarnings('ignore')

DATA_PATH = 'C:/â€¦â€¦'
SAVE_IMPORTANCE_PATH = 'C:/â€¦â€¦'
SAVE_PLOT_PATH = 'C:/â€¦â€¦'
SAVE_VALIDATION_SUMMARY = 'C:/â€¦â€¦'

os.makedirs(SAVE_PLOT_PATH, exist_ok=True)

GROUP_FEATURES_CONFIG = {
    'ä½åŸå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
    'ä½åŸå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
    'ä¸­åŸå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
    'ä¸­åŸå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
    'é«˜åŸå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
    'é«˜åŸå¸‚åŒ–ç»„_LA85': [â€¦â€¦]
}

ID_VARS = ['pac', 'Year', 'Urb']
TARGET_VARS = ['LA65', 'LA85']
URBAN_GROUPS = ['ä½åŸå¸‚åŒ–ç»„', 'ä¸­åŸå¸‚åŒ–ç»„', 'é«˜åŸå¸‚åŒ–ç»„']
TARGET_YEARS = [2010, 2020]  
QUANTILE_THRESHOLD = 0.01

RF_PARAMS = {
    'n_estimators': 100,
    'max_depth': 10,
    'min_samples_split': 5,
    'random_state': 42,
    'n_jobs': -1,
    'oob_score': True 
}

REPEAT_TIMES = 3
RANDOM_SEEDS = [42, 123, 456]

def load_data(path):

    df = pd.read_excel(path)
    print(f"åŸå§‹æ•°æ®åˆ—å: {df.columns.tolist()}")
    print(f"æ£€æŸ¥æ˜¯å¦æœ‰'Year'åˆ—: {'Year' in df.columns}")
    print(f"æ£€æŸ¥æ˜¯å¦æœ‰'Urb'åˆ—: {'Urb' in df.columns}")

    if 'Year' in df.columns:
        df = df[df['Year'].isin(TARGET_YEARS)]
        print(f"ç­›é€‰2010/2020å¹´æ•°æ®åï¼Œæ ·æœ¬é‡: {len(df)}")
        print(f"å„å¹´ä»½æ ·æœ¬é‡: {df['Year'].value_counts()}")
    else:
        raise KeyError("æ•°æ®ä¸­æ²¡æœ‰'Year'åˆ—ï¼Œæ— æ³•åˆ†å¹´åº¦å¤„ç†")

    if 'Urb' in df.columns:
        df['Urb_Group'] = pd.cut(df['Urb'], bins=[-1, 3, 7, 10], labels=URBAN_GROUPS)
        print(f"åŸå¸‚åŒ–åˆ†ç»„åˆ›å»ºå®Œæˆï¼Œå„ç»„æ ·æœ¬é‡:")
        print(df['Urb_Group'].value_counts())
    else:
        print("é”™è¯¯ï¼šæ•°æ®ä¸­æ²¡æœ‰'Urb'åˆ—")
        possible_cols = [col for col in df.columns if 'urb' in col.lower() or 'åŸå¸‚åŒ–' in col]
        if possible_cols:
            print(f"æ‰¾åˆ°å¯èƒ½çš„åˆ—: {possible_cols}")
            df['Urb_Group'] = pd.cut(df[possible_cols[0]], bins=[-1, 3, 7, 10], labels=URBAN_GROUPS)
        else:
            raise KeyError("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°åŸå¸‚åŒ–ç›¸å…³çš„åˆ—")

    print(f"åˆ†å¹´åº¦æ•°æ®åŠ è½½å®Œæˆï¼šå…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")
    return df


def handle_outliers(df, features, group_col='Urb_Group', year_col='Year', quantile=0.01):

    df_out = df.copy()
    if group_col not in df_out.columns or year_col not in df_out.columns:
        print(f"è­¦å‘Šï¼šç¼ºå°‘åˆ†ç»„/å¹´ä»½åˆ—ï¼Œè·³è¿‡åˆ†ç»„å¤„ç†")
        return df_out

    for group in df_out[group_col].unique():
        if pd.isna(group):
            continue
        for year in TARGET_YEARS:
            mask = (df_out[group_col] == group) & (df_out[year_col] == year)
            if not mask.any():
                continue
            for feat in features:
                if feat not in df_out.columns:
                    continue
                group_year_data = df_out.loc[mask, feat]
                if len(group_year_data) > 0:
                    lower = group_year_data.quantile(quantile)
                    upper = group_year_data.quantile(1 - quantile)
                    df_out.loc[mask, feat] = group_year_data.clip(lower=lower, upper=upper)
    print("æç«¯å€¼ç¼©å°¾å¤„ç†å®Œæˆï¼ˆæŒ‰åˆ†ç»„+å¹´ä»½ï¼‰")
    return df_out


def impute_missing(df, features):

    numeric_features = [f for f in features if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]
    if len(numeric_features) == 0:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æ•°å€¼å‹ç‰¹å¾è¿›è¡Œæ’è¡¥")
        return df

    df_imputed = df.copy()

    for year in TARGET_YEARS:
        year_mask = df_imputed['Year'] == year
        if not year_mask.any():
            continue
        year_data = df_imputed[year_mask].copy()
        imputer = IterativeImputer(random_state=42, max_iter=10)
        df_imputed.loc[year_mask, numeric_features] = imputer.fit_transform(year_data[numeric_features])
    print(f"ç¼ºå¤±å€¼è¿­ä»£æ’è¡¥å®Œæˆï¼Œå¤„ç†äº†{len(numeric_features)}ä¸ªç‰¹å¾ï¼ˆæŒ‰å¹´ä»½ï¼‰")
    return df_imputed

def calculate_fitting_metrics(model, X, y):

    y_pred = model.predict(X)
    r2 = np.round(r2_score(y, y_pred), 4).item()
    mae = np.round(mean_absolute_error(y, y_pred), 4).item()
    mse = np.round(mean_squared_error(y, y_pred), 4).item()
    return {'è®­ç»ƒé›†RÂ²': r2, 'MAE': mae, 'MSE': mse}


def calculate_importance_consistency(X, y, features, n_repeats=REPEAT_TIMES, seeds=RANDOM_SEEDS):

    importance_rank_list = []
    for seed in seeds:
        params = RF_PARAMS.copy()
        params['random_state'] = seed
        rf = RandomForestRegressor(**params)
        rf.fit(X, y)
        imp_df = pd.DataFrame({'å˜é‡å': features, 'é‡è¦æ€§': rf.feature_importances_})
        imp_df = imp_df.sort_values('é‡è¦æ€§', ascending=False).reset_index(drop=True)
        importance_rank_list.append(imp_df['å˜é‡å'].tolist())

    consistency_scores = []
    for i in range(n_repeats):
        for j in range(i + 1, n_repeats):
            rank_i = pd.Series(range(1, len(features) + 1), index=importance_rank_list[i])
            rank_j = pd.Series(range(1, len(features) + 1), index=importance_rank_list[j])

            rank_i_vals = rank_i[features].values
            rank_j_vals = rank_j[features].values
            valid_mask = ~(np.isnan(rank_i_vals) | np.isnan(rank_j_vals))

            if not np.any(valid_mask):
                corr = np.nan
            else:
                res = spearmanr(rank_i_vals[valid_mask], rank_j_vals[valid_mask])
                corr = res.statistic

            consistency_scores.append(np.round(corr, 4).item() if not np.isnan(corr) else np.nan)

    valid_scores = [s for s in consistency_scores if not np.isnan(s)]
    avg_consistency = np.round(np.mean(valid_scores), 4).item() if valid_scores else np.nan
    return {'é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰': avg_consistency}

def train_yearly_rf_with_validation(df, target):
    all_importance = []
    validation_summary = []

    for group in URBAN_GROUPS:

        config_key = f'{group}_{target}'
        if config_key not in GROUP_FEATURES_CONFIG:
            print(f"âš ï¸ æ— {config_key}çš„ç‰¹å¾é…ç½®ï¼Œè·³è¿‡è¯¥åˆ†ç»„")
            continue
        target_features = GROUP_FEATURES_CONFIG[config_key]
        print(f"\n=== å¤„ç†{config_key}ï¼Œä½¿ç”¨ç‰¹å¾: {target_features} ===")

        group_data = df[df['Urb_Group'] == group].copy()
        if len(group_data) < 30:
            print(f"âš ï¸ {group}ï¼ˆ{target}ï¼‰æ ·æœ¬é‡ä¸è¶³{len(group_data)}ï¼Œè·³è¿‡")
            continue

        for year in TARGET_YEARS:
            year_data = group_data[group_data['Year'] == year].copy()
            if len(year_data) < 20:  
                print(f"âš ï¸ {group}ï¼ˆ{target}ï¼‰{year}å¹´æ ·æœ¬é‡ä¸è¶³{len(year_data)}ï¼Œè·³è¿‡")
                continue

            available_features = [f for f in target_features if f in year_data.columns]
            missing_features = [f for f in target_features if f not in year_data.columns]
            if missing_features:
                print(f"âš ï¸ {group}ï¼ˆ{target}ï¼‰{year}å¹´ç¼ºå¤±ç‰¹å¾: {missing_features}")
            if len(available_features) == 0:
                print(f"âš ï¸ {group}ï¼ˆ{target}ï¼‰{year}å¹´æ— å¯ç”¨ç‰¹å¾ï¼Œè·³è¿‡")
                continue

            X = year_data[available_features].copy()
            y = year_data[target].copy()

            rf_year = RandomForestRegressor(**RF_PARAMS)
            rf_year.fit(X, y)

            oob_r2 = np.round(rf_year.oob_score_, 4).item() if (
                    hasattr(rf_year, 'oob_score_') and not np.isnan(rf_year.oob_score_)) else np.nan
            fitting_metrics = calculate_fitting_metrics(rf_year, X, y)
            consistency_metrics = calculate_importance_consistency(X, y, available_features)

            imp_df = pd.DataFrame({
                'åŸå¸‚åŒ–åˆ†ç»„': group,
                'å› å˜é‡': target,
                'å¹´ä»½': year,
                'å˜é‡å': available_features,
                'å˜é‡é‡è¦æ€§': rf_year.feature_importances_,
                'é‡è¦æ€§å æ¯”(%)': np.round(rf_year.feature_importances_ * 100, 2),
                'OOB-RÂ²': oob_r2,
                'è®­ç»ƒé›†RÂ²': fitting_metrics['è®­ç»ƒé›†RÂ²'],
                'MAE': fitting_metrics['MAE']
            })
            imp_df = imp_df.sort_values('å˜é‡é‡è¦æ€§', ascending=False).reset_index(drop=True)
            all_importance.append(imp_df)

            val_summary = {
                'åŸå¸‚åŒ–åˆ†ç»„': group,
                'å› å˜é‡': target,
                'å¹´ä»½': year,
                'æ ·æœ¬é‡': len(year_data),
                'ä½¿ç”¨ç‰¹å¾æ•°': len(available_features),
                'ç¼ºå¤±ç‰¹å¾': ','.join(missing_features) if missing_features else 'æ— ',
                'OOB-RÂ²': oob_r2,
                'è®­ç»ƒé›†RÂ²': fitting_metrics['è®­ç»ƒé›†RÂ²'],
                'MAE': fitting_metrics['MAE'],
                'MSE': fitting_metrics['MSE'],
                'å˜é‡é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰': consistency_metrics['é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰']
            }
            validation_summary.append(val_summary)

            plot_yearly_importance(imp_df.head(10), group, target, year)
            print(
                f"âœ… {group}ï¼ˆ{target}ï¼‰{year}å¹´è®­ç»ƒå®Œæˆï¼šOOB-RÂ²={oob_r2}ï¼Œæ’åºä¸€è‡´æ€§={val_summary['å˜é‡é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰']}")

    return pd.concat(all_importance, ignore_index=True) if all_importance else pd.DataFrame(), \
        pd.DataFrame(validation_summary) if validation_summary else pd.DataFrame()


def plot_yearly_importance(importance, group, target, year):

    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.figure(figsize=(10, 6))
    bars = plt.barh(importance['å˜é‡å'][::-1], importance['é‡è¦æ€§å æ¯”(%)'][::-1], color='#ff7f0e')
    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.1, bar.get_y() + bar.get_height() / 2,
                 f'{width:.1f}%', ha='left', va='center', fontsize=9)
    plt.title(f'Variable Importance (Year {year}) - {group} ({target})', fontsize=14, fontweight='bold')
    plt.xlabel('Importance (%)', fontsize=12)
    plt.ylabel('Variables', fontsize=12)
    plt.tight_layout()

    safe_group = group.replace('/', '_').replace('\\', '_')
    safe_target = target.replace('/', '_').replace('\\', '_')
    filename = f'{SAVE_PLOT_PATH}{safe_group}_{safe_target}_{year}_importance.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"åˆ†å¹´åº¦å›¾è¡¨å·²ä¿å­˜: {filename}")

def main_yearly_rf():
    df = load_data(DATA_PATH)
    print(f"\næ•°æ®æ¦‚è§ˆ: {df.shape}")

    all_importance = []
    all_validation = []

    for target in TARGET_VARS:
        print(f"\n=== å¼€å§‹å¤„ç†å› å˜é‡ï¼š{target} ===")
        if target not in df.columns:
            print(f"âš ï¸ ç›®æ ‡å˜é‡{target}ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue

        all_config_features = list(set([f for feat_list in GROUP_FEATURES_CONFIG.values() for f in feat_list]))
        df = handle_outliers(df, all_config_features)
        df = impute_missing(df, all_config_features)

        imp_df, val_df = train_yearly_rf_with_validation(df, target)
        if not imp_df.empty:
            all_importance.append(imp_df)
        if not val_df.empty:
            all_validation.append(val_df)

    if all_importance:
        final_importance = pd.concat(all_importance, ignore_index=True)
        final_importance.to_excel(SAVE_IMPORTANCE_PATH, index=False)
        print(f"\nğŸ‰ åˆ†å¹´åº¦å˜é‡é‡è¦æ€§ç»“æœå·²ä¿å­˜è‡³ï¼š{SAVE_IMPORTANCE_PATH}")
    else:
        print("\nâš ï¸ æ²¡æœ‰ç”Ÿæˆåˆ†å¹´åº¦å˜é‡é‡è¦æ€§ç»“æœ")

    if all_validation:
        final_validation = pd.concat(all_validation, ignore_index=True)
        final_validation.to_excel(SAVE_VALIDATION_SUMMARY, index=False)
        print(f"ğŸ‰ åˆ†å¹´åº¦æ¨¡å‹éªŒè¯æ±‡æ€»è¡¨å·²ä¿å­˜è‡³ï¼š{SAVE_VALIDATION_SUMMARY}")
        print("\n=== åˆ†å¹´åº¦æ¨¡å‹éªŒè¯æ±‡æ€»é¢„è§ˆ ===")
        print(final_validation[['åŸå¸‚åŒ–åˆ†ç»„', 'å› å˜é‡', 'å¹´ä»½', 'æ ·æœ¬é‡', 'OOB-RÂ²', 'è®­ç»ƒé›†RÂ²',
                                'å˜é‡é‡è¦æ€§æ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰']].head())
    else:
        print("\nâš ï¸ æœªç”Ÿæˆåˆ†å¹´åº¦æ¨¡å‹éªŒè¯æ±‡æ€»è¡¨")


if __name__ == '__main__':
    main_yearly_rf()
