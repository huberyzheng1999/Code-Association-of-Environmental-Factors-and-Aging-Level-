import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from scipy.stats import spearmanr
import warnings
import os

warnings.filterwarnings('ignore')

DATA_PATH = 'C:/â€¦â€¦'
SAVE_IMPORTANCE_PATH = 'C:/â€¦â€¦'
SAVE_PLOT_PATH = 'C:/â€¦â€¦'
SAVE_VALIDATION_SUMMARY = 'C:/â€¦â€¦'

os.makedirs(SAVE_PLOT_PATH, exist_ok=True)

GROUP_FEATURES_CONFIG = {
    2010: {
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
        'é«˜åŸŽå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
        'é«˜åŸŽå¸‚åŒ–ç»„_LA85': [â€¦â€¦]
    },
    2020: {
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA85': [â€¦â€¦],
        'é«˜åŸŽå¸‚åŒ–ç»„_LA65': [â€¦â€¦],
        'é«˜åŸŽå¸‚åŒ–ç»„_LA85': [â€¦â€¦]
    }
}

YEAR_GROUP_SAMPLE = {
    2010: {'ä½ŽåŸŽå¸‚åŒ–ç»„': 1005, 'ä¸­åŸŽå¸‚åŒ–ç»„': 1229, 'é«˜åŸŽå¸‚åŒ–ç»„': 620},
    2020: {'ä½ŽåŸŽå¸‚åŒ–ç»„': 953, 'ä¸­åŸŽå¸‚åŒ–ç»„': 1208, 'é«˜åŸŽå¸‚åŒ–ç»„': 674}
}

YEAR_GROUP_TARGET_PARAMS = {
    2010: {
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA65': {'n_estimators': 400, 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 7,
                            'max_features': 0.6, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA85': {'n_estimators': 400, 'max_depth': 10, 'min_samples_split': 14, 'min_samples_leaf': 6,
                            'max_features': 0.6, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA65': {'n_estimators': 500, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 5,
                            'max_features': 0.5, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA85': {'n_estimators': 450, 'max_depth': 9, 'min_samples_split': 12, 'min_samples_leaf': 5,
                            'max_features': 0.5, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'é«˜åŸŽå¸‚åŒ–ç»„_LA65': {'n_estimators': 500, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 4,
                            'max_features': 0.5, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'é«˜åŸŽå¸‚åŒ–ç»„_LA85': {'n_estimators': 400, 'max_depth': 8, 'min_samples_split': 14, 'min_samples_leaf': 7,
                            'max_features': 0.6, 'random_state': 42, 'n_jobs': -1, 'oob_score': True}
    },

    2020: {
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA65': {'n_estimators': 300, 'max_depth': 11, 'min_samples_split': 7, 'min_samples_leaf': 3,
                            'max_features': 0.75, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'ä½ŽåŸŽå¸‚åŒ–ç»„_LA85': {'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 3,
                            'max_features': 0.75, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA65': {'n_estimators': 350, 'max_depth': 9, 'min_samples_split': 13, 'min_samples_leaf': 6,
                            'max_features': 0.65, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'ä¸­åŸŽå¸‚åŒ–ç»„_LA85': {'n_estimators': 350, 'max_depth': 9, 'min_samples_split': 14, 'min_samples_leaf': 6,
                            'max_features': 0.65, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'é«˜åŸŽå¸‚åŒ–ç»„_LA65': {'n_estimators': 800, 'max_depth': 7, 'min_samples_split': 13, 'min_samples_leaf': 6,
                            'max_features': 0.6, 'random_state': 42, 'n_jobs': -1, 'oob_score': True},
        'é«˜åŸŽå¸‚åŒ–ç»„_LA85': {'n_estimators': 850, 'max_depth': 8, 'min_samples_split': 14, 'min_samples_leaf': 5,
                            'max_features': 0.65, 'random_state': 42, 'n_jobs': -1, 'oob_score': True}
    }
}

ID_VARS = ['pac', 'Year', 'Urb']
TARGET_VARS = ['LA65', 'LA85']
URBAN_GROUPS = ['ä½ŽåŸŽå¸‚åŒ–ç»„', 'ä¸­åŸŽå¸‚åŒ–ç»„', 'é«˜åŸŽå¸‚åŒ–ç»„']
TARGET_YEARS = [2010, 2020]
QUANTILE_THRESHOLD = 0.01

REPEAT_TIMES = 3
RANDOM_SEEDS = [42, 123, 456]
MIN_TRAIN_R2 = 0.5       
MAX_OVERFIT_GAP = 0.2   
MIN_OOB_R2 = 0.4     

def load_data(path):
    df = pd.read_excel(path)
    print(f"åŽŸå§‹æ•°æ®åˆ—å: {df.columns.tolist()}")
    print(f"æ£€æŸ¥æ˜¯å¦æœ‰'Year'åˆ—: {'Year' in df.columns}")
    print(f"æ£€æŸ¥æ˜¯å¦æœ‰'Urb'åˆ—: {'Urb' in df.columns}")

    if 'Year' in df.columns:
        df = df[df['Year'].isin(TARGET_YEARS)]
        print(f"ç­›é€‰2010/2020å¹´æ•°æ®åŽï¼Œæ ·æœ¬é‡: {len(df)}")
        print(f"å„å¹´ä»½æ ·æœ¬é‡: {df['Year'].value_counts()}")
    else:
        raise KeyError("æ•°æ®ä¸­æ²¡æœ‰'Year'åˆ—ï¼Œæ— æ³•åˆ†å¹´åº¦å¤„ç†")

    if 'Urb' in df.columns:
        df['Urb_Group'] = pd.cut(df['Urb'], bins=[-1, 3, 7, 10], labels=URBAN_GROUPS)
        print(f"åŸŽå¸‚åŒ–åˆ†ç»„åˆ›å»ºå®Œæˆï¼Œå„ç»„æ ·æœ¬é‡:")
        print(df['Urb_Group'].value_counts())
    else:
        print("é”™è¯¯ï¼šæ•°æ®ä¸­æ²¡æœ‰'Urb'åˆ—")
        possible_cols = [col for col in df.columns if 'urb' in col.lower() or 'åŸŽå¸‚åŒ–' in col]
        if possible_cols:
            print(f"æ‰¾åˆ°å¯èƒ½çš„åˆ—: {possible_cols}")
            df['Urb_Group'] = pd.cut(df[possible_cols[0]], bins=[-1, 3, 7, 10], labels=URBAN_GROUPS)
        else:
            raise KeyError("æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°åŸŽå¸‚åŒ–ç›¸å…³çš„åˆ—")

    for year in TARGET_YEARS:
        year_df = df[df['Year'] == year]
        for group in URBAN_GROUPS:
            actual = len(year_df[year_df['Urb_Group'] == group])
            expected = YEAR_GROUP_SAMPLE[year][group]
            print(f"{year}å¹´{group}ï¼šé¢„æœŸæ ·æœ¬é‡{expected}ï¼Œå®žé™…{actual}ï¼ˆåå·®{abs(actual - expected)}ï¼‰")

    print(f"åˆ†å¹´åº¦æ•°æ®åŠ è½½å®Œæˆï¼šå…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")
    return df


def handle_outliers(df, features, group_col='Urb_Group', year_col='Year', quantile=0.01):

    df_out = df.copy()
    if group_col not in df_out.columns or year_col not in df_out.columns:
        print(f"è­¦å‘Šï¼šç¼ºå°‘åˆ†ç»„/å¹´ä»½åˆ—ï¼Œè·³è¿‡åˆ†ç»„å¤„ç†")
        return df_out

    for group in df_out[group_col].unique():
        if pd.isna(group):
            continue
        for year in TARGET_YEARS:
            mask = (df_out[group_col] == group) & (df_out[year_col] == year)
            if not mask.any():
                continue
            for feat in features:
                if feat not in df_out.columns:
                    continue
                group_year_data = df_out.loc[mask, feat]
                if len(group_year_data) > 0:
                    lower = group_year_data.quantile(quantile)
                    upper = group_year_data.quantile(1 - quantile)
                    df_out.loc[mask, feat] = group_year_data.clip(lower=lower, upper=upper)
    print("æžç«¯å€¼ç¼©å°¾å¤„ç†å®Œæˆï¼ˆæŒ‰åˆ†ç»„+å¹´ä»½ï¼‰")
    return df_out


def impute_missing(df, features):
    numeric_features = [f for f in features if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]
    if len(numeric_features) == 0:
        print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æ•°å€¼åž‹ç‰¹å¾è¿›è¡Œæ’è¡¥")
        return df

    df_imputed = df.copy()
    for year in TARGET_YEARS:
        year_mask = df_imputed['Year'] == year
        if not year_mask.any():
            continue
        year_data = df_imputed[year_mask].copy()
        imputer = IterativeImputer(random_state=42, max_iter=10)
        df_imputed.loc[year_mask, numeric_features] = imputer.fit_transform(year_data[numeric_features])
    print(f"ç¼ºå¤±å€¼è¿­ä»£æ’è¡¥å®Œæˆï¼Œå¤„ç†äº†{len(numeric_features)}ä¸ªç‰¹å¾ï¼ˆæŒ‰å¹´ä»½ï¼‰")
    return df_imputed

def calculate_fitting_metrics(model, X, y):
    y_pred = model.predict(X)
    train_r2 = np.round(r2_score(y, y_pred), 4).item()
    oob_r2 = np.round(model.oob_score_, 4).item() if (
                hasattr(model, 'oob_score_') and not np.isnan(model.oob_score_)) else np.nan
    overfit_gap = np.round(train_r2 - oob_r2, 4).item() if not np.isnan(oob_r2) else np.nan

    r2_pass = train_r2 > MIN_TRAIN_R2
    oob_pass = oob_r2 >= MIN_OOB_R2 if not np.isnan(oob_r2) else False
    overfit_pass = overfit_gap < MAX_OVERFIT_GAP if not np.isnan(overfit_gap) else False

    if r2_pass and oob_pass and overfit_pass:
        print(f"âœ… å…¨è¾¾æ ‡ï¼šè®­ç»ƒé›†RÂ²={train_r2}(ï¼ž{MIN_TRAIN_R2})ï¼ŒOOB-RÂ²={oob_r2}(â‰¥{MIN_OOB_R2})ï¼Œè¿‡æ‹Ÿåˆå·®å€¼={overfit_gap}(ï¼œ{MAX_OVERFIT_GAP})")
    else:
        fail_reason = []
        if not r2_pass:
            fail_reason.append(f"è®­ç»ƒé›†RÂ²={train_r2}(â‰¤{MIN_TRAIN_R2})")
        if not oob_pass:
            fail_reason.append(f"OOB-RÂ²={oob_r2}(ï¼œ{MIN_OOB_R2})")
        if not overfit_pass:
            fail_reason.append(f"è¿‡æ‹Ÿåˆå·®å€¼={overfit_gap}(â‰¥{MAX_OVERFIT_GAP})")
        print(f"âš ï¸ æœªè¾¾æ ‡ï¼š{' | '.join(fail_reason)}")

    mae = np.round(mean_absolute_error(y, y_pred), 4).item()
    mse = np.round(mean_squared_error(y, y_pred), 4).item()
    return {
        'è®­ç»ƒé›†RÂ²': train_r2,
        'OOB-RÂ²': oob_r2,
        'è¿‡æ‹Ÿåˆå·®å€¼': overfit_gap,
        'MAE': mae,
        'MSE': mse,
        'RÂ²è¾¾æ ‡': r2_pass,
        'OOBè¾¾æ ‡': oob_pass,
        'è¿‡æ‹Ÿåˆè¾¾æ ‡': overfit_pass
    }


def adjust_params_dynamically(params, r2_pass, oob_pass, overfit_pass):
    new_params = params.copy()
    if not r2_pass and oob_pass and overfit_pass:
        new_params['max_depth'] += 1 if new_params['max_depth'] < 20 else 0
        new_params['min_samples_split'] = max(2, new_params['min_samples_split'] - 1)
        new_params['min_samples_leaf'] = max(1, new_params['min_samples_leaf'] - 1)
        new_params['max_features'] = min(1.0, new_params['max_features'] + 0.05)
        print(f"ðŸ“Œ åŠ¨æ€æ”¾å®½å‚æ•°ï¼ˆæå‡è®­ç»ƒé›†RÂ²ï¼‰ï¼š{new_params}")
    elif r2_pass and not oob_pass and overfit_pass:
        new_params['max_depth'] += 1 if new_params['max_depth'] < 20 else 0
        new_params['min_samples_split'] = max(2, new_params['min_samples_split'] - 1)
        new_params['max_features'] = min(1.0, new_params['max_features'] + 0.05)
        print(f"ðŸ“Œ åŠ¨æ€æ”¾å®½å‚æ•°ï¼ˆæå‡OOB-RÂ²ï¼‰ï¼š{new_params}")
    elif r2_pass and oob_pass and not overfit_pass:
        new_params['max_depth'] = max(3, new_params['max_depth'] - 1)
        new_params['min_samples_split'] += 1
        new_params['min_samples_leaf'] = max(1, new_params['min_samples_leaf'] + 1)
        new_params['max_features'] = max(0.3, new_params['max_features'] - 0.05)
        print(f"ðŸ“Œ åŠ¨æ€æ”¶ç´§å‚æ•°ï¼ˆæŽ§åˆ¶è¿‡æ‹Ÿåˆï¼‰ï¼š{new_params}")
    elif not (r2_pass and oob_pass and overfit_pass):
        new_params['max_depth'] += 1 if new_params['max_depth'] < 20 else 0
        new_params['min_samples_split'] = max(2, new_params['min_samples_split'] - 1)
        new_params['min_samples_leaf'] = max(1, new_params['min_samples_leaf'])
        new_params['max_features'] = min(0.95, new_params['max_features'] + 0.05)
        print(f"ðŸ“Œ åŠ¨æ€å¹³è¡¡å‚æ•°ï¼ˆå…¼é¡¾RÂ²/OOB/è¿‡æ‹Ÿåˆï¼‰ï¼š{new_params}")
    return new_params

def calculate_importance_consistency(X, y, features, year, group, target):
    importance_rank_list = []
    for seed in RANDOM_SEEDS:
        config_key = f'{group}_{target}'
        params = YEAR_GROUP_TARGET_PARAMS[year][config_key].copy()
        params['random_state'] = seed
        rf = RandomForestRegressor(**params)
        rf.fit(X, y)
        imp_df = pd.DataFrame({'å˜é‡å': features, 'å˜é‡é‡è¦æ€§': rf.feature_importances_})
        imp_df = imp_df.sort_values('å˜é‡é‡è¦æ€§', ascending=False).reset_index(drop=True)
        importance_rank_list.append(imp_df['å˜é‡å'].tolist())

    consistency_scores = []
    for i in range(REPEAT_TIMES):
        for j in range(i + 1, REPEAT_TIMES):
            rank_i = pd.Series(range(1, len(features) + 1), index=importance_rank_list[i])
            rank_j = pd.Series(range(1, len(features) + 1), index=importance_rank_list[j])

            rank_i_vals = rank_i[features].values
            rank_j_vals = rank_j[features].values
            valid_mask = ~(np.isnan(rank_i_vals) | np.isnan(rank_j_vals))

            if not np.any(valid_mask):
                corr = np.nan
            else:
                res = spearmanr(rank_i_vals[valid_mask], rank_j_vals[valid_mask])
                corr = res.statistic

            consistency_scores.append(np.round(corr, 4).item() if not np.isnan(corr) else np.nan)

    valid_scores = [s for s in consistency_scores if not np.isnan(s)]
    avg_consistency = np.round(np.mean(valid_scores), 4).item() if valid_scores else np.nan
    return {'é‡è¦æ€§æŽ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰': avg_consistency}

def train_yearly_rf_with_validation(df, target):
    all_importance = []
    validation_summary = []

    for year in TARGET_YEARS:
        print(f"\n==== å¤„ç†{year}å¹´ {target} ====")
        year_data = df[df['Year'] == year].copy()
        if len(year_data) == 0:
            print(f"âš ï¸ {year}å¹´æ— æ•°æ®ï¼Œè·³è¿‡")
            continue

        for group in URBAN_GROUPS:
            config_key = f'{group}_{target}'
            if config_key not in GROUP_FEATURES_CONFIG[year]:
                print(f"âš ï¸ æ— {year}å¹´{config_key}çš„ç‰¹å¾é…ç½®ï¼Œè·³è¿‡")
                continue
            if config_key not in YEAR_GROUP_TARGET_PARAMS[year]:
                print(f"âš ï¸ æ— {year}å¹´{config_key}çš„å‚æ•°é…ç½®ï¼Œè·³è¿‡")
                continue

            target_features = GROUP_FEATURES_CONFIG[year][config_key]
            init_params = YEAR_GROUP_TARGET_PARAMS[year][config_key].copy()
            print(f"\n--- å¤„ç†{year}å¹´{group} {target}ï¼Œé…ç½®ç‰¹å¾æ•°ï¼š{len(target_features)} ---")
            print(f"åˆå§‹å‚æ•°ï¼š{init_params}")

            group_year_data = year_data[year_data['Urb_Group'] == group].copy()
            actual_sample = len(group_year_data)
            expected_sample = YEAR_GROUP_SAMPLE[year][group]
            if actual_sample < 20:  # æœ€å°æ ·æœ¬é‡é˜ˆå€¼
                print(f"âš ï¸ {year}å¹´{group}æ ·æœ¬é‡{actual_sample}ï¼œ20ï¼Œè·³è¿‡")
                continue
            print(f"{year}å¹´{group}æ ·æœ¬é‡ï¼šå®žé™…{actual_sample}ï¼ˆé¢„æœŸ{expected_sample}ï¼‰")

            available_features = [f for f in target_features if f in group_year_data.columns]
            missing_features = [f for f in target_features if f not in group_year_data.columns]
            if missing_features:
                print(f"âš ï¸ ç¼ºå¤±ç‰¹å¾: {missing_features}")
            if len(available_features) == 0:
                print(f"âš ï¸ æ— å¯ç”¨ç‰¹å¾ï¼Œè·³è¿‡")
                continue

            X = group_year_data[available_features].copy()
            y = group_year_data[target].copy()

            final_params = init_params.copy()
            metrics = None
            adjust_times = 0
            max_adjust = 2  # æœ€å¤§è°ƒæ•´æ¬¡æ•°

            while adjust_times < max_adjust:

                rf_year = RandomForestRegressor(**final_params)
                rf_year.fit(X, y)

                metrics = calculate_fitting_metrics(rf_year, X, y)

                if metrics['RÂ²è¾¾æ ‡'] and metrics['OOBè¾¾æ ‡'] and metrics['è¿‡æ‹Ÿåˆè¾¾æ ‡']:
                    break

                final_params = adjust_params_dynamically(final_params, metrics['RÂ²è¾¾æ ‡'], metrics['OOBè¾¾æ ‡'], metrics['è¿‡æ‹Ÿåˆè¾¾æ ‡'])
                adjust_times += 1
                print(f"ç¬¬{adjust_times}æ¬¡è°ƒæ•´åŽé‡æ–°è®­ç»ƒ...")

            consistency_metrics = calculate_importance_consistency(X, y, available_features, year, group, target)

            imp_df = pd.DataFrame({
                'å¹´ä»½': year,
                'åŸŽå¸‚åŒ–åˆ†ç»„': group,
                'å› å˜é‡': target,
                'å˜é‡å': available_features,
                'å˜é‡é‡è¦æ€§': rf_year.feature_importances_,
                'é‡è¦æ€§å æ¯”(%)': np.round(rf_year.feature_importances_ * 100, 2),
                'OOB-RÂ²': metrics['OOB-RÂ²'],
                'è®­ç»ƒé›†RÂ²': metrics['è®­ç»ƒé›†RÂ²'],
                'è¿‡æ‹Ÿåˆå·®å€¼': metrics['è¿‡æ‹Ÿåˆå·®å€¼'],
                'MAE': metrics['MAE'],
                'RÂ²è¾¾æ ‡': metrics['RÂ²è¾¾æ ‡'],
                'OOBè¾¾æ ‡': metrics['OOBè¾¾æ ‡'],
                'è¿‡æ‹Ÿåˆè¾¾æ ‡': metrics['è¿‡æ‹Ÿåˆè¾¾æ ‡']
            })
            imp_df = imp_df.sort_values('å˜é‡é‡è¦æ€§', ascending=False).reset_index(drop=True)
            all_importance.append(imp_df)

            val_summary = {
                'å¹´ä»½': year,
                'åŸŽå¸‚åŒ–åˆ†ç»„': group,
                'å› å˜é‡': target,
                'é¢„æœŸæ ·æœ¬é‡': expected_sample,
                'å®žé™…æ ·æœ¬é‡': actual_sample,
                'é…ç½®ç‰¹å¾æ•°': len(target_features),
                'ä½¿ç”¨ç‰¹å¾æ•°': len(available_features),
                'ç¼ºå¤±ç‰¹å¾': ','.join(missing_features) if missing_features else 'æ— ',
                'OOB-RÂ²': metrics['OOB-RÂ²'],
                'è®­ç»ƒé›†RÂ²': metrics['è®­ç»ƒé›†RÂ²'],
                'è¿‡æ‹Ÿåˆå·®å€¼': metrics['è¿‡æ‹Ÿåˆå·®å€¼'],
                'MAE': metrics['MAE'],
                'MSE': metrics['MSE'],
                'å˜é‡é‡è¦æ€§æŽ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰': consistency_metrics['é‡è¦æ€§æŽ’åºä¸€è‡´æ€§ï¼ˆå¹³å‡Spearmanï¼‰'],
                'æœ€ç»ˆä½¿ç”¨å‚æ•°': str(final_params),
                'RÂ²è¾¾æ ‡': metrics['RÂ²è¾¾æ ‡'],
                'OOBè¾¾æ ‡': metrics['OOBè¾¾æ ‡'],
                'è¿‡æ‹Ÿåˆè¾¾æ ‡': metrics['è¿‡æ‹Ÿåˆè¾¾æ ‡'],
                'è°ƒæ•´æ¬¡æ•°': adjust_times
            }
            validation_summary.append(val_summary)

            plot_yearly_importance(imp_df.head(10), group, target, year)
            print(f"âœ… {year}å¹´{group} {target}è®­ç»ƒå®Œæˆï¼šæœ€ç»ˆRÂ²={metrics['è®­ç»ƒé›†RÂ²']}ï¼ŒOOB-RÂ²={metrics['OOB-RÂ²']}ï¼Œè¿‡æ‹Ÿåˆå·®å€¼={metrics['è¿‡æ‹Ÿåˆå·®å€¼']}")

    return pd.concat(all_importance, ignore_index=True) if all_importance else pd.DataFrame(), \
        pd.DataFrame(validation_summary) if validation_summary else pd.DataFrame()


def plot_yearly_importance(importance, group, target, year):

    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.figure(figsize=(10, 6))
    bars = plt.barh(importance['å˜é‡å'][::-1], importance['é‡è¦æ€§å æ¯”(%)'][::-1], color='#ff7f0e')
    for bar in bars:
        width = bar.get_width()
        plt.text(width + 0.1, bar.get_y() + bar.get_height() / 2,
                 f'{width:.1f}%', ha='left', va='center', fontsize=9)
    plt.title(f'Variable Importance (Year {year}) - {group} ({target})', fontsize=14, fontweight='bold')
    plt.xlabel('Importance (%)', fontsize=12)
    plt.ylabel('Variables', fontsize=12)
    plt.tight_layout()

    safe_group = group.replace('/', '_').replace('\\', '_')
    safe_target = target.replace('/', '_').replace('\\', '_')
    filename = f'{SAVE_PLOT_PATH}{year}_{safe_group}_{safe_target}_importance.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"åˆ†å¹´åº¦å›¾è¡¨å·²ä¿å­˜: {filename}")

def main_yearly_rf():
    df = load_data(DATA_PATH)
    print(f"\næ•°æ®æ¦‚è§ˆ: {df.shape}")

    all_config_features = []
    for year in TARGET_YEARS:
        for feat_list in GROUP_FEATURES_CONFIG[year].values():
            all_config_features.extend(feat_list)
    all_config_features = list(set(all_config_features))
    print(f"\næ‰€æœ‰é…ç½®çš„ç‰¹å¾æ€»æ•°ï¼š{len(all_config_features)}")
    print(f"é…ç½®ç‰¹å¾åˆ—è¡¨ï¼š{all_config_features}")

    df = handle_outliers(df, all_config_features)
    df = impute_missing(df, all_config_features)

    all_importance = []
    all_validation = []

    for target in TARGET_VARS:
        print(f"\n=== å¼€å§‹å¤„ç†å› å˜é‡ï¼š{target} ===")
        if target not in df.columns:
            print(f"âš ï¸ ç›®æ ‡å˜é‡{target}ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue

        imp_df, val_df = train_yearly_rf_with_validation(df, target)
        if not imp_df.empty:
            all_importance.append(imp_df)
        if not val_df.empty:
            all_validation.append(val_df)

    if all_importance:
        final_importance = pd.concat(all_importance, ignore_index=True)
        final_importance.to_excel(SAVE_IMPORTANCE_PATH, index=False)
        print(f"\nðŸŽ‰ åˆ†å¹´åº¦å˜é‡é‡è¦æ€§ç»“æžœå·²ä¿å­˜è‡³ï¼š{SAVE_IMPORTANCE_PATH}")

    if all_validation:
        final_validation = pd.concat(all_validation, ignore_index=True)
        final_validation.to_excel(SAVE_VALIDATION_SUMMARY, index=False)
        print(f"ðŸŽ‰ åˆ†å¹´åº¦æ¨¡åž‹éªŒè¯æ±‡æ€»è¡¨å·²ä¿å­˜è‡³ï¼š{SAVE_VALIDATION_SUMMARY}")

        print("\n=== æœ€ç»ˆè¾¾æ ‡æƒ…å†µæ±‡æ€» ===")
        pass_summary = final_validation.groupby(['å¹´ä»½', 'åŸŽå¸‚åŒ–åˆ†ç»„', 'å› å˜é‡'])[['RÂ²è¾¾æ ‡', 'OOBè¾¾æ ‡', 'è¿‡æ‹Ÿåˆè¾¾æ ‡']].first()
        pass_summary['å…¨è¾¾æ ‡'] = pass_summary['RÂ²è¾¾æ ‡'] & pass_summary['OOBè¾¾æ ‡'] & pass_summary['è¿‡æ‹Ÿåˆè¾¾æ ‡']
        print(pass_summary)

        failed = final_validation[~(final_validation['RÂ²è¾¾æ ‡'] & final_validation['OOBè¾¾æ ‡'] & final_validation['è¿‡æ‹Ÿåˆè¾¾æ ‡'])]
        if not failed.empty:
            print("\nâŒ æœªè¾¾æ ‡é¡¹ï¼š")
            print(failed[['å¹´ä»½', 'åŸŽå¸‚åŒ–åˆ†ç»„', 'å› å˜é‡', 'è®­ç»ƒé›†RÂ²', 'OOB-RÂ²', 'è¿‡æ‹Ÿåˆå·®å€¼']])
        else:
            print("\nâœ… æ‰€æœ‰åˆ†ç»„å‡è¾¾æ ‡ï¼ˆè®­ç»ƒé›†RÂ²ï¼ž0.5ï¼ŒOOB-RÂ²â‰¥0.4ï¼Œè¿‡æ‹Ÿåˆå·®å€¼ï¼œ0.2ï¼‰")


if __name__ == '__main__':
    main_yearly_rf()
